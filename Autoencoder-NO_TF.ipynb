{
    "metadata": {
        "language_info": {
            "file_extension": ".py", 
            "mimetype": "text/x-python", 
            "version": "2.7.11", 
            "pygments_lexer": "ipython2", 
            "name": "python", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "display_name": "Python 2 with Spark 1.6", 
            "language": "python", 
            "name": "python2"
        }
    }, 
    "cells": [
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": 1, 
            "source": "# A bit of setup\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n# for auto-reloading extenrnal modules\n# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n%load_ext autoreload\n%autoreload 2"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "metadata": {}, 
                    "execution_count": 2, 
                    "data": {
                        "text/plain": "   Material  Usura_mat_g  Durezza_Disco  Comprex_LP  Comprex_LR  Grindo_LP  \\\n0  1.000000     0.900208       0.880503    0.673423    0.677704   0.129032   \n1  0.997764     0.919958       0.805031    0.675676    0.631347   0.206452   \n2  0.996452     0.980249       0.440252    0.801802    0.779249   0.218280   \n3  0.996452     0.977131       0.427673    0.774775    0.854305   0.246237   \n4  0.995529     0.944906       0.786164    0.641892    0.635762   0.118280   \n\n   Grindo_LR  PistoneCpx   Inerzia      Vmax    ...          183       184  \\\n0   0.062284           1  0.272727  0.434783    ...     0.539481  0.534210   \n1   0.159170           1  0.272727  0.434783    ...     0.675487  0.691069   \n2   0.156863           1  0.272727  0.434783    ...     0.539481  0.534210   \n3   0.222607           1  0.272727  0.434783    ...     0.539481  0.534210   \n4   0.071511           1  0.272727  0.434783    ...     0.613546  0.546417   \n\n        185       186       187       188       189       190       191  \\\n0  0.740648  0.438179  0.516201  0.320636  0.623794  0.298908  0.689731   \n1  0.701569  0.363226  0.450055  0.331382  0.711773  0.343537  0.808774   \n2  0.740648  0.438179  0.516201  0.320636  0.623794  0.298908  0.689731   \n3  0.740648  0.438179  0.516201  0.320636  0.623794  0.298908  0.689731   \n4  0.741318  0.444433  0.520374  0.323024  0.615110  0.295961  0.698293   \n\n        192  \n0  0.515836  \n1  0.616340  \n2  0.515836  \n3  0.515836  \n4  0.475325  \n\n[5 rows x 235 columns]", 
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Material</th>\n      <th>Usura_mat_g</th>\n      <th>Durezza_Disco</th>\n      <th>Comprex_LP</th>\n      <th>Comprex_LR</th>\n      <th>Grindo_LP</th>\n      <th>Grindo_LR</th>\n      <th>PistoneCpx</th>\n      <th>Inerzia</th>\n      <th>Vmax</th>\n      <th>...</th>\n      <th>183</th>\n      <th>184</th>\n      <th>185</th>\n      <th>186</th>\n      <th>187</th>\n      <th>188</th>\n      <th>189</th>\n      <th>190</th>\n      <th>191</th>\n      <th>192</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.000000</td>\n      <td>0.900208</td>\n      <td>0.880503</td>\n      <td>0.673423</td>\n      <td>0.677704</td>\n      <td>0.129032</td>\n      <td>0.062284</td>\n      <td>1</td>\n      <td>0.272727</td>\n      <td>0.434783</td>\n      <td>...</td>\n      <td>0.539481</td>\n      <td>0.534210</td>\n      <td>0.740648</td>\n      <td>0.438179</td>\n      <td>0.516201</td>\n      <td>0.320636</td>\n      <td>0.623794</td>\n      <td>0.298908</td>\n      <td>0.689731</td>\n      <td>0.515836</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.997764</td>\n      <td>0.919958</td>\n      <td>0.805031</td>\n      <td>0.675676</td>\n      <td>0.631347</td>\n      <td>0.206452</td>\n      <td>0.159170</td>\n      <td>1</td>\n      <td>0.272727</td>\n      <td>0.434783</td>\n      <td>...</td>\n      <td>0.675487</td>\n      <td>0.691069</td>\n      <td>0.701569</td>\n      <td>0.363226</td>\n      <td>0.450055</td>\n      <td>0.331382</td>\n      <td>0.711773</td>\n      <td>0.343537</td>\n      <td>0.808774</td>\n      <td>0.616340</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.996452</td>\n      <td>0.980249</td>\n      <td>0.440252</td>\n      <td>0.801802</td>\n      <td>0.779249</td>\n      <td>0.218280</td>\n      <td>0.156863</td>\n      <td>1</td>\n      <td>0.272727</td>\n      <td>0.434783</td>\n      <td>...</td>\n      <td>0.539481</td>\n      <td>0.534210</td>\n      <td>0.740648</td>\n      <td>0.438179</td>\n      <td>0.516201</td>\n      <td>0.320636</td>\n      <td>0.623794</td>\n      <td>0.298908</td>\n      <td>0.689731</td>\n      <td>0.515836</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.996452</td>\n      <td>0.977131</td>\n      <td>0.427673</td>\n      <td>0.774775</td>\n      <td>0.854305</td>\n      <td>0.246237</td>\n      <td>0.222607</td>\n      <td>1</td>\n      <td>0.272727</td>\n      <td>0.434783</td>\n      <td>...</td>\n      <td>0.539481</td>\n      <td>0.534210</td>\n      <td>0.740648</td>\n      <td>0.438179</td>\n      <td>0.516201</td>\n      <td>0.320636</td>\n      <td>0.623794</td>\n      <td>0.298908</td>\n      <td>0.689731</td>\n      <td>0.515836</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.995529</td>\n      <td>0.944906</td>\n      <td>0.786164</td>\n      <td>0.641892</td>\n      <td>0.635762</td>\n      <td>0.118280</td>\n      <td>0.071511</td>\n      <td>1</td>\n      <td>0.272727</td>\n      <td>0.434783</td>\n      <td>...</td>\n      <td>0.613546</td>\n      <td>0.546417</td>\n      <td>0.741318</td>\n      <td>0.444433</td>\n      <td>0.520374</td>\n      <td>0.323024</td>\n      <td>0.615110</td>\n      <td>0.295961</td>\n      <td>0.698293</td>\n      <td>0.475325</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 235 columns</p>\n</div>"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "execution_count": 2, 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Variable definition and assignment:\nShape of input matrix X: (651, 235)\nShape of target vector y: (651,)\nNumber of target classes K: 651\nNumber of features D: 235\nNumber of samples num_samples: 651\nNumber of hidden units h: 150\nShape of the input-hidden weights matrix W: (235, 150)\nShape of the input-hidden bias b: (1, 150)\nShape of the hidden-output weights matrix W2: (150, 651)\nShape of the hidden-output bias b2: (1, 651)\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": null, 
            "source": "print \"Variable definition and assignment:\"\n\narray_data_1 = df_data_1.values\ny_data_1 = df_data_1.index\n\nD =  array_data_1.shape[1]# number of features\nK = array_data_1.shape[0] # number of classes\nX = array_data_1\ny = df_data_1.index\nnum_examples = array_data_1.shape[0]\n\nprint \"Shape of input matrix X: \" + str(X.shape)\nprint \"Shape of target vector y: \" + str(y.shape)\nprint \"Number of target classes K: \" + str(K)\nprint \"Number of features D: \" + str(D)\nprint \"Number of samples num_samples: \" + str(num_examples)\n\nh = 150 # size of hidden layer, the new number of features per sample\nW = 0.01 * np.random.randn(D,h)\nb = np.zeros((1,h))\nW2 = 0.01 * np.random.randn(h,K)\nb2 = np.zeros((1,K))\n\nprint \"Number of hidden units h: \" + str(h)\nprint \"Shape of the input-hidden weights matrix W: \" + str(W.shape)\nprint \"Shape of the input-hidden bias b: \" + str(b.shape)\nprint \"Shape of the hidden-output weights matrix W2: \" + str(W2.shape)\nprint \"Shape of the hidden-output bias b2: \" + str(b2.shape)"
        }, 
        {
            "metadata": {
                "scrolled": false, 
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "iteration 0: loss 6.485269\n", 
                    "name": "stdout"
                }
            ], 
            "execution_count": null, 
            "source": "# some hyperparameters\nstep_size = 1e-2\nreg = 1e-3 # regularization strength\ntraining_losses = []\n\nfor i in xrange(200000):\n  \n    # evaluate class scores, [N x K]\n    hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n    scores = np.dot(hidden_layer, W2) + b2\n  \n    # compute the class probabilities\n    exp_scores = np.exp(scores)\n    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # computing for each sample the probability of being a certain sample\n  \n    # compute the loss: average cross-entropy loss and regularization\n    corect_logprobs = -np.log(probs[range(num_examples),y])\n    data_loss = np.sum(corect_logprobs)/num_examples\n    reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n    loss = data_loss + reg_loss\n    training_losses.append(loss)\n    \n    if i % 1000 == 0:\n        print \"iteration %d: loss %f\" % (i, loss)\n  \n    # compute the gradient on scores\n    dscores = probs\n    dscores[range(num_examples),y] -= 1\n    dscores /= num_examples\n  \n    # backpropate the gradient to the parameters\n    # first backprop into parameters W2 and b2\n    dW2 = np.dot(hidden_layer.T, dscores)\n    db2 = np.sum(dscores, axis=0, keepdims=True)\n    # next backprop into hidden layer\n    dhidden = np.dot(dscores, W2.T)\n    # backprop the ReLU non-linearity\n    dhidden[hidden_layer <= 0] = 0\n    # finally into W,b\n    dW = np.dot(X.T, dhidden)\n    db = np.sum(dhidden, axis=0, keepdims=True)\n  \n    # add regularization gradient contribution\n    dW2 += reg * W2\n    dW += reg * W\n  \n    # perform a parameter update\n    W += -step_size * dW\n    b += -step_size * db\n    W2 += -step_size * dW2\n    b2 += -step_size * db2\n\nplt.plot(training_losses)\nplt.title(\"Plot loss trend:\", fontsize=30)\nplt.xlabel(\"iteration\", fontsize=20)\nplt.ylabel(\"loss\", fontsize=20)"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "# evaluate training set accuracy\nhidden_layer = np.maximum(0, np.dot(X, W) + b)\nscores = np.dot(hidden_layer, W2) + b2\npredicted_class = np.argmax(scores, axis=1)\nprint 'training accuracy: %.2f' % (np.mean(predicted_class == y))"
        }
    ], 
    "nbformat": 4, 
    "nbformat_minor": 0
}